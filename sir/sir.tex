\documentclass{article}
\usepackage{graphicx}
\usepackage{float}
\usepackage{fullpage}
\usepackage{array}
\usepackage{pdflscape}
\usepackage{multirow}
\usepackage{bibentry}
%\nobibliography*
%\usepackage[T1]{fontenc}
%\usepackage{lscape}
%\usepackage[parfill]{parskip}

\setlength{\extrarowheight}{4pt}

\graphicspath{{./images/}}

\floatstyle{boxed}
\restylefloat{figure}

\begin{document}
\input{title.tex}
\tableofcontents
\listoffigures
%\listoftables

\section{Purpose and Scope of Implementation} % Zach
% 10%
% State the purpose of the document and its intended audience. Refer to the
% requirements and design documents.

% Summarize what portion of the design was actually implemented in the
% prototype. Describe the main use cases supported by the implementation. List
% any significant requirements not implemented due to time constraints.

\section{Changes to the Design} % Zach
% 10%
% Summarize any changes that were required to the design specified in the
% Design Document, and jusity the design deviations.

\section{Transformation to Implementation} % Ben
% 10%
% Summarize how the design was mapped to the implementation. Consider the
% process model, process communication, storage schema, and other areas were
% implementation decisions were made.

The classes of the system were mapped from the class diagrams in the UML of the
design document in figures of Chapter~5. 

The controllers are run on separate threads so that they can all respond to
events in a timely manner. The event manager, which the controllers and devices
use to communicate through, is run on two separate threads, the main thread and
an additional one, in the same process as the controllers. This was to allow
for the event manager to be able to enqueue events from the controllers without
blocking sending of the events to the controllers.

Database storage was omitted from this release, but there is a simple log of
the events stored with the controllers in JSON format. This log can be easily
parsed into a database with a script once a database has been implemented into
the system.

The process model was kept simple. The controllers talked to the event manager.
The event manager and the devices talked to each other through sockets. A
sensor console user interface was created to assist in sending sensor events
through the sockets to the event manager and eventually the controllers. The
system controller handled what to do for each event type. If an alarm event
needed to be fired off, then the system controller instantiated it and sent it
to the event manager to be sent to the alarm devices and alarm controller.
Every event handled by the system controller was sent to a web server through
JSON-RPC. 

A web server maintained a list of events in memory. These events were fetched
by the two mobile applications for iOS and Android platforms. Events are
viewable on either application depending on what the user has. The iOS
application, due to the overhead required to properly store and display data,
was written in a similar manner to the system itself. Each event type is
represented as a class with proper usage of inheritance. The Android
application, with it's more flexible libraries, was able to use a list data
structure and display the data without first creating models. Both applications
can be easily extended as required by future needs.

\section{Testing Strategy} % Casey 
% 10%
% Identify the general approach taken to testing. For example, discuss the
% manner in which subsystems were tested individually through unit testing and
% in which the system was tested as a whole through integration testing.
% Identify the main subsystems and interfaces included in the test plan.
% Describe the test environment, including emulation of hardware devices.

Several testing strategies were employed during the development of
GARTH. Firstly, test-driven development was used during the initial
phases of development and any time new classes or functionality was
added. Secondly, larger tests were written that verified
interoperability of the various GARTH components. Finally, a sensor
test console program was created for integration testing purposes.

Test-driven development was used to ease development by reducing the
risk of introducing bugs when adding new code and changing existing
code. In this strategy, when a developer was about to add some new
functionality, a unit test for that functionality was added
first. This way, the developer can interpret the software spec once,
write the test, then concentrate on adding the functionality to pass
the test. This reduces the time that the developer spends context
switching between reading the software specification and the code
itself. Another added benefit of this testing strategy was that every
line of code in the project was executed at some point by the test
suite; this means that any refactor that was made could easily be
verified by simply re-running the test suite.

Larger automated tests were written that verified that the different
parts of GARTH communicated correctly. Any time that a communication
channel between two components was implemented by the developer, an
automated test for that communications channel was added. For example,
when socket connections were added to the communications stack,
automated tests were added that verified that the data sent via
sockets was correct. Having automated tests with a larger scope
allowed for large refactors to be made during development with minimal
risk.

Finally, in order to test the system's behaviour in response to the
various inputs it could receive, a test sensor console program was
created. This program consisted of a GUI with buttons to trigger each
type of Event in the system, as well as text fields to customize the
specific fields in each Event subclass. This sensor console was used
to verify that the logic in the various state machines was correct. The console was also used during the project demo to demonstrate the high level functionality of the system.

\section{Error Handling} 
% 10%
% Describe potential and realistic faults that could occur in the
% implementation and how the system prevents, detects, mitigates, and/or
% recovers from failure. For instance, describe whether the implementation
% performs error checking on user input, recovers from interrupted
% commmunication, or handles component failure.

\section{Test Results}
% 20%
% Summarize what testing techniques were performed on the implementation.
% Consider tests that pertain to functionality, interfaces, boundary
% conditions, resource handling, performance, load handling, and system
% integration. Do not include full test case descriptions, but rather, describe
% the tests that were executed in general terms.

\section{Outstanding Issues} % Zach
% 10%
% List any outstanding faults, deficiences, or missing features, and make
% recommendations on how they should be addressed in the final version.

\section{Build and Installation Instructions}
% 10%
% Provide a brief outline of the steps required for compiling, installing, and
% running the implementation given a copy of the submitted source code. List
% any development tools and libraries that are required.

\section{Extensibility}
% 10%
% Recommend any functional and non-functional improvments to make in the final
% version of the implementation to be released or a subsequent version. These
% may address use cases that have already been described, or have been
% considered following completion of the Design Document. For instance,
% consider whether the system may be made more reliable, perform better, scale
% to more users, function with new types of components, or be more accessible.
% Give specific examples consistant with the present implementation.

\end{document}
